# ‚úÖ Migration vers ZeroGPU - Compl√®te!

## üéØ Changements effectu√©s

Le projet a √©t√© migr√© de **Mistral API** vers **Qwen3-4B-Instruct avec ZeroGPU**.

### Avant (API Mistral)
- ‚ùå N√©cessitait une cl√© API Mistral (payant)
- ‚ùå D√©pendance externe (API cloud)
- ‚ùå Latence r√©seau
- ‚úÖ Pas besoin de GPU

### Apr√®s (ZeroGPU + Qwen3)
- ‚úÖ **Pas de cl√© API n√©cessaire** (gratuit sur HF Spaces)
- ‚úÖ Mod√®le open source (Qwen3-4B-Instruct)
- ‚úÖ ZeroGPU pour acc√©l√©ration automatique
- ‚úÖ Inf√©rence rapide avec GPU
- ‚úÖ Compatible avec les Spaces Qwen3 existants

## üìã Fichiers modifi√©s

### 1. Backend AI
#### Cr√©√©
- `src/ai/qwen_zerogpu_analyzer.py` - Nouveau backend avec @spaces.GPU

#### Supprim√©
- `src/ai/mistral_api_analyzer.py` - Ancien backend API Mistral

### 2. Interface UI
#### Modifi√©
- `src/ui/spaces_interface.py`
  - Import: `QwenZeroGPUAnalyzer` au lieu de `MistralAPIAnalyzer`
  - Messages d'erreur adapt√©s
  - Footer mis √† jour

### 3. Configuration
#### Modifi√©
- `requirements.txt`
  ```diff
  - mistralai>=1.2.0
  - python-dotenv>=1.0.0
  - httpx>=0.24.0
  + transformers>=4.35.0
  + torch>=2.0.0
  + accelerate>=0.24.0
  + spaces>=0.28.0
  ```

- `README.md`
  ```yaml
  hardware: zero-gpu  # Ajout√©
  models:
  - Qwen/Qwen3-4B-Instruct  # Ajout√©
  tags:
  - qwen
  - qwen3
  - zero-gpu
  ```

### 4. D√©ploiement
#### Modifi√©
- `deploy.py`
  - `set_space_secrets()` - Plus besoin de MISTRAL_API_KEY
  - Validation: v√©rifie `qwen_zerogpu_analyzer.py`

- `.env.example`
  - Aucune cl√© API requise

### 5. Documentation
#### Modifi√©
- `QUICK_START.md` - 2 √©tapes au lieu de 3 (pas de cl√© API)
- `README.md` - Sections mises √† jour avec Qwen + ZeroGPU

### 6. Tests
#### Modifi√©
- `test_local.py`
  - V√©rifie CUDA/MPS au lieu de MISTRAL_API_KEY
  - Note sur ZeroGPU uniquement disponible sur HF Spaces

## üîß Architecture technique

### D√©corateur @spaces.GPU

```python
@spaces.GPU(duration=60)  # Max 60 secondes sur ZeroGPU
def generate_response(self, conversation, max_tokens=4000):
    # Le mod√®le est automatiquement charg√© sur GPU
    # La m√©moire GPU est g√©r√©e automatiquement
    # Apr√®s 60s, le GPU est lib√©r√©
```

### Chargement du mod√®le

```python
def _load_model(self):
    """Charg√© une seule fois, au premier appel"""
    self.tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen3-4B-Instruct",
        trust_remote_code=True
    )

    self.model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen3-4B-Instruct",
        torch_dtype=torch.bfloat16,
        device_map="auto",  # ZeroGPU g√®re l'allocation
        trust_remote_code=True
    )
```

### G√©n√©ration

```python
# Chat template Qwen
prompt = tokenizer.apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True
)

# G√©n√©ration avec GPU
outputs = model.generate(
    **inputs,
    max_new_tokens=4000,
    temperature=0.2,
    do_sample=False,  # Greedy pour diagrammes d√©terministes
    pad_token_id=tokenizer.eos_token_id
)
```

## üöÄ D√©ploiement

### Avant (Mistral API)
```bash
export HF_TOKEN="xxx"
export MISTRAL_API_KEY="xxx"  # ‚ùå N√©cessaire
python3 deploy.py
```

### Apr√®s (ZeroGPU)
```bash
export HF_TOKEN="xxx"
python3 deploy.py  # ‚úÖ C'est tout!
```

## üß™ Tests

### En local (CPU/GPU si disponible)
```bash
cd huggingface-space
pip install -r requirements.txt
python3 test_local.py
```

**Note**: ZeroGPU n'est pas disponible en local. Le test local utilise CPU ou GPU CUDA/MPS si disponible.

### Sur HF Spaces (ZeroGPU)
```bash
python3 deploy.py
```

L'app sera d√©ploy√©e avec ZeroGPU automatiquement!

## üìä Comparaison des performances

| Aspect | Mistral API | Qwen + ZeroGPU |
|--------|-------------|----------------|
| **Co√ªt** | Pay-per-use | Gratuit (HF Spaces) |
| **Latence** | ~2-5s (r√©seau) | ~1-3s (GPU local) |
| **Setup** | Cl√© API requise | Aucune config |
| **Mod√®le** | Mistral (cloud) | Qwen3-4B (open source) |
| **GPU** | N/A | ZeroGPU (T4/A10G) |
| **Timeout** | N/A | 60s max par requ√™te |

## ‚úÖ Checklist de migration

- [x] Backend Qwen cr√©√© (`qwen_zerogpu_analyzer.py`)
- [x] Interface UI adapt√©e
- [x] Requirements mis √† jour
- [x] README metadata mis √† jour (hardware: zero-gpu)
- [x] Deploy script adapt√©
- [x] Documentation mise √† jour
- [x] Tests locaux adapt√©s
- [x] Ancien code Mistral supprim√©
- [x] .env.example mis √† jour

## üéâ Pr√™t pour le d√©ploiement!

Le projet est maintenant configur√© pour utiliser **Qwen3-4B-Instruct avec ZeroGPU**, compatible avec les Spaces Qwen3!

**Commande de d√©ploiement** :
```bash
export HF_TOKEN="hf_xxxxxxxxxxxxx"
cd huggingface-space
python3 deploy.py
```

**URL du Space** (apr√®s d√©ploiement) :
```
https://huggingface.co/spaces/VincentGOURBIN/PVB-Flow-Mermaid-Generator
```

---

üìñ **Documentation** : Voir [QUICK_START.md](./QUICK_START.md) pour le guide rapide
